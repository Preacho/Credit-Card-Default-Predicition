{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40aaa785",
   "metadata": {
    "id": "cLzD25UY9Xzc"
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from itertools import combinations\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# clustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# clustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "#Outlier\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "#Feature Selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "\n",
    "#Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve\n",
    "\n",
    "#Hyperparameter Tuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "\n",
    "# outlier detection\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# feature selection\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "# classification\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2181b1df",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "id": "HbMNwokl8-Fn",
    "outputId": "2abfb044-796f-48d0-adb9-71b68442522c"
   },
   "outputs": [],
   "source": [
    "# upload UCI_Credit_Card.csv to files\n",
    "data = pd.read_csv('UCI_Credit_Card.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d4c1da",
   "metadata": {
    "id": "BESDb5ka-FRd"
   },
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb21683",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Oj9Iqv85-HXe",
    "outputId": "93e8328e-518d-46d9-a025-13357266af8f"
   },
   "outputs": [],
   "source": [
    "# check if any missing information is present in a csv file.\n",
    "\n",
    "#Number of rows before\n",
    "print(f\"Number of Rows before cleaning: {data.shape[0]}\")\n",
    "\n",
    "#Clean data\n",
    "data = data.dropna()\n",
    "\n",
    "#Check number of rows after:\n",
    "\n",
    "print(f\"Number of Rows after cleaning: {data.shape[0]}\")\n",
    "\n",
    "# should we also remove 0 values for certain attributes?\n",
    "# Looking at the scatter plots, it looks like some values of 0 make analysis\n",
    "# harder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605f7a09",
   "metadata": {
    "id": "01tDVAvK-WkM"
   },
   "source": [
    "There are no values in the dataset that are missing in the form of NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293f6e7e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BIoEpkoL-KSa",
    "lines_to_next_cell": 2,
    "outputId": "35438e20-1563-4c89-c9a0-6c5ad694df21"
   },
   "outputs": [],
   "source": [
    "# Drop duplicates\n",
    "#Number of rows before\n",
    "print(f\"Number of rows before removing duplicates: {data.shape[0]}\")\n",
    "\n",
    "duplicate_count = data.duplicated().sum()\n",
    "\n",
    "print(f\"Number of duplicates: {duplicate_count}\")\n",
    "\n",
    "if(duplicate_count == 0):\n",
    "  print(\"no duplicates\")\n",
    "\n",
    "else:\n",
    "  data = data.drop_duplicates()\n",
    "  print(f\"Number of rows after removing duplicates: {data.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca21f1bf",
   "metadata": {
    "id": "d9Z8eoiw-asq"
   },
   "source": [
    "We have dropped data rows that do not make sense (Example: Marriage that is listed as 1 = married, 2 = single, 3 = others, but some rows had 0. We were unsure what this meant so the rows that had it were droppped.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f2026d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SHdWKv6l-bs5",
    "outputId": "001ddb68-a3c0-4e3a-cfbf-3d8e9c8c5c4d"
   },
   "outputs": [],
   "source": [
    "#Drop odd values in categorical values\n",
    "print(f\"Number of rows before removing odd rows: {data.shape[0]}\")\n",
    "\n",
    "data = data[data['MARRIAGE'].isin([1,2,3])]\n",
    "data = data[data['EDUCATION'].isin([1,2,3,4,5,6])]\n",
    "data = data[data['SEX'].isin([1,2])]\n",
    "\n",
    "pay_list = ['PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']\n",
    "\n",
    "for pay in pay_list:\n",
    "  # -2 is assumed to be no credit to pay\n",
    "  # -1 is credit paid fully\n",
    "  # 0 is assumed to be payment made on time (minimum satement)\n",
    "  data = data[data[pay].isin(range(-2,10))]\n",
    "\n",
    "\n",
    "print(f\"Number of rows after removing odd rows: {data.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c57317",
   "metadata": {
    "id": "OZfu2O5c-0ZJ",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# one-hot encoding of categorical features\n",
    "cat_cols = ['MARRIAGE']\n",
    "data[cat_cols] = data[cat_cols].astype('category')\n",
    "data = pd.get_dummies(data, columns=cat_cols, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285d206c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HyFKGXIf-1W9",
    "outputId": "214db2d2-fecf-44ab-d61e-098f9080db94"
   },
   "outputs": [],
   "source": [
    "# See new column names\n",
    "print(data.columns.tolist())\n",
    "\n",
    "# Compare before/after shape\n",
    "print(\"Shape after encoding:\", data.shape)\n",
    "\n",
    "data.head()\n",
    "\n",
    "data.to_csv('preprocessed_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be42f265",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 795
    },
    "id": "Tm0RYnShElot",
    "outputId": "74a59ff7-3d3c-430d-a618-22d0b4533c92"
   },
   "outputs": [],
   "source": [
    "# Data augmentation generating synthetic samples\n",
    "# use SMOTE to solve class imbalance problem\n",
    "x = data.drop('default.payment.next.month', axis=1)\n",
    "y = data['default.payment.next.month']\n",
    "smote=SMOTE(sampling_strategy='minority', random_state=42)\n",
    "x,y=smote.fit_resample(x,y)\n",
    "plt.figure(figsize=(7,5))\n",
    "sns.countplot(x=y)\n",
    "plt.title('Class Distribution (0: No Default, 1: Default) after SMOTE')\n",
    "plt.xlabel('Default Payment Next Month')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "print(y.value_counts())\n",
    "data = pd.concat([x, y], axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effc8279",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "id": "OVcV_JB9Eo0h",
    "outputId": "bcbedcc0-2fcf-480d-fe47-0ef200c813f1"
   },
   "outputs": [],
   "source": [
    "# Normalization/Standardization of data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "num_cols = ['LIMIT_BAL','AGE'] + [f'BILL_AMT{i}' for i in range(1,7)] + [f'PAY_AMT{i}' for i in range(1,7)]\n",
    "data[num_cols].describe().T[['mean', 'std']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb1d4a9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "id": "k6fTmGRGE2ok",
    "outputId": "e0f3e440-fa56-4503-e506-65edc787828c"
   },
   "outputs": [],
   "source": [
    "data[num_cols] = scaler.fit_transform(data[num_cols])\n",
    "data[num_cols].describe().T[['mean', 'std']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8341457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save preprocessed data to csv file\n",
    "data.to_csv('preprocessed_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e59fbec",
   "metadata": {},
   "source": [
    "### Read Preprocseed Data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e1417f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('preprocessed_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae21e2a1",
   "metadata": {
    "id": "pHJKwRwe9el-"
   },
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65457cf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 653
    },
    "id": "cuUTXKQkvs-O",
    "lines_to_next_cell": 2,
    "outputId": "dcd7090b-525f-43f7-a265-ddfe24a2bc04"
   },
   "outputs": [],
   "source": [
    "# Find optimal number of components using BIC and AIC\n",
    "n_components = np.arange(1, 15)\n",
    "bic_scores = []\n",
    "aic_scores = []\n",
    "\n",
    "for n in n_components:\n",
    "    gmm = GaussianMixture(n_components=n, random_state=42)\n",
    "    gmm.fit(data)\n",
    "    bic_scores.append(gmm.bic(data))\n",
    "    aic_scores.append(gmm.aic(data))\n",
    "\n",
    "# Plot the BIC and AIC scores\n",
    "plt.figure(figsize=(18, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(n_components, bic_scores, marker='o')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('BIC Score')\n",
    "plt.title('BIC Score vs. Number of Components')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(n_components, aic_scores, marker='x')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('AIC Score')\n",
    "plt.title('AIC Score vs. Number of Components')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e87e63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up components needed for clustering\n",
    "X_clustering = data.drop(['ID', 'default.payment.next.month'], axis=1)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca_components = pca.fit_transform(X_clustering)\n",
    "\n",
    "tsne = TSNE(n_components=2)\n",
    "tsne_components = tsne.fit_transform(X_clustering)\n",
    "\n",
    "def visualize_cluster(components, cluster_labels, title):\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.scatterplot(x=components[:, 0], y=components[:, 1], hue=cluster_labels, palette='viridis', s=50)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Component 1')\n",
    "    plt.ylabel('Component 2')\n",
    "    plt.legend(title='Cluster')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f205d229",
   "metadata": {
    "id": "XGGwFLLkupOj"
   },
   "source": [
    "Clustering Method 1: Probabilistic Clustering - GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49afabaf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "wNSj9Q3Jt2hA",
    "outputId": "cabe7dda-9822-4406-a856-8c4c0fa160cf"
   },
   "outputs": [],
   "source": [
    "# 1. Setup Data (Drop Target and ID for clustering)\n",
    "# Ensure 'data' is your scaled/preprocessed dataframe\n",
    "X_clustering = data.drop(['ID', 'default.payment.next.month'], axis=1)\n",
    "\n",
    "# 2. Fit GMM\n",
    "optimal_components = 2  # You can adjust this based on your BIC/AIC plots\n",
    "gmm = GaussianMixture(n_components=optimal_components, random_state=42)\n",
    "cluster_labels = gmm.fit_predict(X_clustering)\n",
    "\n",
    "# 3. Assign Labels back to a copy of data for analysis\n",
    "df_clusters = data.copy()\n",
    "df_clusters['cluster'] = cluster_labels\n",
    "\n",
    "# 4. Evaluate Performance\n",
    "print(\"Clustering Performance Metrics:\")\n",
    "print(f\"Silhouette Score: {silhouette_score(X_clustering, cluster_labels):.4f}\")\n",
    "print(f\"Calinski-Harabasz Index: {calinski_harabasz_score(X_clustering, cluster_labels):.4f}\")\n",
    "print(f\"Davies-Bouldin Index: {davies_bouldin_score(X_clustering, cluster_labels):.4f}\")\n",
    "\n",
    "# 5. Visualize using PCA (2D)\n",
    "pca = PCA(n_components=2)\n",
    "pca_components = pca.fit_transform(X_clustering)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.scatterplot(x=pca_components[:, 0], y=pca_components[:, 1], hue=cluster_labels, palette='viridis', s=50)\n",
    "plt.title('GMM Clustering Results (PCA Reduced)')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.legend(title='Cluster')\n",
    "plt.show()\n",
    "\n",
    "# 6. Cluster Means Summary (Fixing your previous error)\n",
    "# Group by the new 'cluster' column we created in df_clusters\n",
    "cluster_summary = df_clusters.groupby('cluster')[X_clustering.columns].mean()\n",
    "print(\"\\nCluster Means:\")\n",
    "print(cluster_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad428ee",
   "metadata": {
    "id": "gPELu3mouyW6"
   },
   "source": [
    "Clustering Method 2: Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd178ced",
   "metadata": {
    "id": "AsdxhG3cu5Tn",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# for this clustering algorithm, the data is too large and must be reduced\n",
    "n_clusters = np.arange(2, 11)\n",
    "linkages = {'ward', 'complete', 'average', 'single'}\n",
    "best_silhouette = -1.0\n",
    "best_labels = None\n",
    "best_n = 2\n",
    "best_link = None\n",
    "\n",
    "# find best model using small sample\n",
    "data_sub = pd.DataFrame(data).sample(20000, random_state=42) # Reduce to 20k samples\n",
    "X_sub = data_sub.drop(['default.payment.next.month', 'ID'], axis=1)\n",
    "y_sub = data_sub['default.payment.next.month']\n",
    "\n",
    "for n in n_clusters:\n",
    "    for l in linkages:\n",
    "        clustering = AgglomerativeClustering(n_clusters=n, linkage=l)\n",
    "        labels = clustering.fit_predict(X_sub)\n",
    "        temp_silhouette_score = silhouette_score(X_sub, labels)\n",
    "        if (temp_silhouette_score > best_silhouette):\n",
    "            best_silhouette = temp_silhouette_score\n",
    "            best_labels = labels\n",
    "            best_n = n\n",
    "            best_link = l\n",
    "\n",
    "print('best number of clusters: ', best_n)\n",
    "print('best linkage criterion: ', best_link)\n",
    "print('silhouette score: ', best_silhouette)\n",
    "\n",
    "chart_X_sub = PCA(2).fit_transform(X_sub)\n",
    "\n",
    "title = f\"Hierarchical Clustering with {best_n} clusters and {best_link} as linkage criterion using PCA for dimensionality reduction\"\n",
    "visualize_cluster(chart_X_sub, best_labels, title)\n",
    "\n",
    "# try with t-SNE\n",
    "tsne_X_sub = TSNE(2).fit_transform(X_sub)\n",
    "title_tsne = f\"Hierarchical Clustering with {best_n} clusters and {best_link} as linkage criterion using t-SNE for dimensionality reduction\"\n",
    "visualize_cluster(tsne_X_sub, best_labels, title_tsne)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f97b0c",
   "metadata": {
    "id": "ijOdsnlLu5pp"
   },
   "source": [
    "Clustering Method 3: K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7402776a",
   "metadata": {
    "id": "ZtDS5r5su-pb"
   },
   "outputs": [],
   "source": [
    "wcss = [] \n",
    "silhouette_scores = []\n",
    "k_range = range(2, 11)\n",
    "X_scaled = data.drop(['ID','default.payment.next.month'], axis=1)\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X_scaled)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "    \n",
    "    # Calculate silhouette score\n",
    "    # Silhouette score requires at least 2 clusters\n",
    "    if k > 1:  \n",
    "        score = silhouette_score(X_scaled, kmeans.labels_)\n",
    "        silhouette_scores.append(score)\n",
    "\n",
    "# Plot Silhouette Scores Method\n",
    "\n",
    "plt.plot(range(2, 11), silhouette_scores, 'ro-')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Scores for Different K values')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#Use Optimal k based on the results from the earlier k findings\n",
    "optimal_k = 3\n",
    "\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
    "kmeans_clusters = kmeans.fit_predict(X_scaled)\n",
    "# Add cluster labels to original dataframe\n",
    "df_with_clusters = data.copy()\n",
    "df_with_clusters['Cluster'] = kmeans_clusters\n",
    "\n",
    "print(\"Cluster distribution:\")\n",
    "print(df_with_clusters['Cluster'].value_counts().sort_index())\n",
    "\n",
    "print(\"Clustering Performance Metrics:\")\n",
    "print(f\"Silhouette Score: {silhouette_score(X_scaled, kmeans_clusters):.4f}\")\n",
    "print(f\"Calinski-Harabasz Index: {calinski_harabasz_score(X_scaled, kmeans_clusters):.4f}\")\n",
    "print(f\"Davies-Bouldin Index: {davies_bouldin_score(X_scaled, kmeans_clusters):.4f}\")\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca_components = pca.fit_transform(X_scaled)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.scatterplot(x=pca_components[:, 0], y=pca_components[:, 1], hue=kmeans_clusters, palette='viridis', s=50)\n",
    "plt.title('K Means Clustering Results (PCA Reduced)')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.legend(title='Cluster')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e3aff5",
   "metadata": {
    "id": "pyt9Hq5Au-8W"
   },
   "source": [
    "# Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d443c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_outliers(components, y_pred_outliers, title):\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    # Plot inliers\n",
    "    plt.scatter(components[y_pred_outliers == 1, 0], components[y_pred_outliers == 1, 1],\n",
    "                c='blue', label='Inliers', s=50, alpha=0.6)\n",
    "    # Plot outliers\n",
    "    plt.scatter(components[y_pred_outliers == -1, 0], components[y_pred_outliers == -1, 1],\n",
    "                c='red', label='Outliers', s=50, marker='x')\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel('PCA Component 1')\n",
    "    plt.ylabel('PCA Component 2')\n",
    "    plt.legend()\n",
    "    plt.show()   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0010203",
   "metadata": {
    "id": "riUSOLmA1mZ4"
   },
   "source": [
    "Outlier Detection Method 1: Local Outlier Factor (LOF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb3d0f6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 676
    },
    "id": "6pnhHITqKBbx",
    "outputId": "58f4f0c2-657c-4b9e-a19f-c35d6ac62fb2"
   },
   "outputs": [],
   "source": [
    "\n",
    "# 1. Create a COPY for outlier analysis (Keeps 'data' pure)\n",
    "df_outliers = data.copy()\n",
    "\n",
    "# 2. Fit LOF\n",
    "# We use X_clustering again since it only has the features we want\n",
    "lof = LocalOutlierFactor(n_neighbors=20, contamination=0.05)\n",
    "y_pred_outliers = lof.fit_predict(X_clustering) # Returns -1 for outliers, 1 for inliers\n",
    "\n",
    "# 3. Store results in the COPY\n",
    "df_outliers['outlier_lof'] = y_pred_outliers\n",
    "\n",
    "# Identify outlier/inlier sets from the copy\n",
    "outliers = df_outliers[df_outliers['outlier_lof'] == -1]\n",
    "inliers = df_outliers[df_outliers['outlier_lof'] == 1]\n",
    "\n",
    "print(f\"Number of outliers detected: {len(outliers)}\")\n",
    "print(f\"Number of inliers: {len(inliers)}\")\n",
    "\n",
    "# 4. Visualize Outliers using PCA (Re-using pca_components from above)\n",
    "plt.figure(figsize=(10, 7))\n",
    "# Plot inliers\n",
    "plt.scatter(pca_components[y_pred_outliers == 1, 0], pca_components[y_pred_outliers == 1, 1],\n",
    "            c='blue', label='Inliers', s=50, alpha=0.6)\n",
    "# Plot outliers\n",
    "plt.scatter(pca_components[y_pred_outliers == -1, 0], pca_components[y_pred_outliers == -1, 1],\n",
    "            c='red', label='Outliers', s=50, marker='x')\n",
    "\n",
    "plt.title('Outlier Detection using LOF (PCA Reduced)')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd39c6d",
   "metadata": {
    "id": "TmLjJ7HI18lv"
   },
   "source": [
    "Outlier Detection Method 2: Isolation Forset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2401b4da",
   "metadata": {
    "id": "5o9uWRIn2GN6",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# I read https://www.geeksforgeeks.org/machine-learning/anomaly-detection-using-isolation-forest/ for help\n",
    "X = data.drop(['default.payment.next.month', 'ID'], axis=1)\n",
    "y = data['default.payment.next.month']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "contaminations = {0.01, 0.05, 0.1, 0.125, 0.15, 0.2}\n",
    "\n",
    "chart_X_test = PCA(2).fit_transform(X_test)\n",
    "tsne_X_test = TSNE(2).fit_transform(X_test)\n",
    "\n",
    "for contamination in contaminations:\n",
    "    clf = IsolationForest(contamination=contamination)\n",
    "    clf.fit(X_train)\n",
    "\n",
    "    y_pred_train = clf.predict(X_train)\n",
    "    y_pred_test = clf.predict(X_test)\n",
    "\n",
    "    # uncomment to see what each algorithm looks like\n",
    "    # title = f\"Isolation Forest with contamination level: {contamination} using PCA\"\n",
    "    # visualize_cluster(chart_X_test, y_pred_test, title)\n",
    "    # title_tsne = f\"Isolation Forest with contamination level: {contamination} using t-SNE\"\n",
    "    # visualize_cluster(tsne_X_test, y_pred_test, title_tsne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df472805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.01 looked the best\n",
    "contamination = 0.01\n",
    "clf = IsolationForest(contamination=contamination)\n",
    "clf.fit(X_train)\n",
    "\n",
    "y_pred_train = clf.predict(X_train)\n",
    "y_pred_test = clf.predict(X_test)\n",
    "\n",
    "title = f\"Isolation Forest with contamination level: {contamination} using PCA\"\n",
    "X_test_pca = PCA(2).fit_transform(X_test)\n",
    "visualize_outliers(X_test_pca, y_pred_test, title)\n",
    "\n",
    "title_tsne = f\"Isolation Forest with contamination level: {contamination} using t-SNE\"\n",
    "X_test_tsne = TSNE(2).fit_transform(X_test)\n",
    "visualize_outliers(X_test_tsne, y_pred_test, title_tsne)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f4e6c1",
   "metadata": {
    "id": "k76nsvqU2Gft"
   },
   "source": [
    "Outlier Detection Method 3: Elliptic Envolope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cde8f8",
   "metadata": {
    "id": "7hjBovv02INT"
   },
   "outputs": [],
   "source": [
    "data_outliers = data.copy()\n",
    "X_elliptic = data_outliers.drop(['ID', 'default.payment.next.month'], axis=1)\n",
    "y_elliptic = data_outliers['default.payment.next.month']\n",
    "\n",
    "outlier_results = {}\n",
    "\n",
    "contamination_levels = [0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1]\n",
    "\n",
    "for contamination in contamination_levels:\n",
    "    print(f\"\\n--- Testing contamination = {contamination} ---\")\n",
    "    \n",
    "    # Initialize Elliptic Envelope\n",
    "    ee = EllipticEnvelope(contamination=contamination, random_state=42)\n",
    "    \n",
    "    # Fit and predict outliers (-1 for outliers, 1 for inliers)\n",
    "    outlier_labels = ee.fit_predict(X_elliptic)\n",
    "    \n",
    "    data_outliers['outlier_ee'] = outlier_labels\n",
    "    outliers_ee = data_outliers[data_outliers['outlier_ee'] == -1]\n",
    "    inliers_ee = data_outliers[data_outliers['outlier_ee'] == 1]\n",
    "\n",
    "    print(f'Number of outliers: {len(outliers_ee)}' )\n",
    "    print(f'Number of inliers: {len(inliers_ee)}')\n",
    "    print(f\"Outlier percentage: {(len(outliers_ee)/len(data_outliers))*100:.2f}%\")\n",
    "\n",
    "    plt.figure(figsize=(10,7))\n",
    "    plt.scatter(pca_components[outlier_labels == 1, 0], pca_components[outlier_labels == 1, 1],\n",
    "                c='blue', label='Inliers', s=50, alpha=0.6)\n",
    "    plt.scatter(pca_components[outlier_labels == -1, 0], pca_components[outlier_labels == -1, 1],\n",
    "                c='red', label='Outliers', s=50,marker='x')\n",
    "    plt.title('Outlier Detection using Elliptic Envelope (PCA Reduced)')\n",
    "    plt.xlabel('PCA Component 1')\n",
    "    plt.ylabel('PCA Component 2')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    data_outliers = data_outliers.drop('outlier_ee', axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cde03ba",
   "metadata": {
    "id": "8_lWkz6P2csa"
   },
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349a09e8",
   "metadata": {
    "id": "cqfQg4Y_2eZO"
   },
   "source": [
    "Feature Selection Method 1: Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea22cf55",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 755
    },
    "id": "55Z4OHvY2izP",
    "outputId": "4f85d6e9-7b51-413e-c2ae-f7a44094df3c"
   },
   "outputs": [],
   "source": [
    "# 1. Prepare Train/Test Data\n",
    "X = data.drop(['ID', 'default.payment.next.month', 'outlier_lof', 'cluster'], axis=1, errors='ignore')\n",
    "y = data['default.payment.next.month']\n",
    "\n",
    "# 2. Apply Lasso (L1 Penalty)\n",
    "# solver='liblinear' is required for L1 penalty\n",
    "lasso_sel = LogisticRegression(penalty='l1', solver='liblinear', C=0.1, random_state=42)\n",
    "lasso_sel.fit(X, y)\n",
    "\n",
    "# 3. Visualize Feature Importance (Coefficients)\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Coefficient': lasso_sel.coef_[0]\n",
    "})\n",
    "# Sort by absolute value of coefficient\n",
    "feature_importance['Abs_Coef'] = feature_importance['Coefficient'].abs()\n",
    "feature_importance = feature_importance.sort_values(by='Abs_Coef', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(x='Coefficient', y='Feature', data=feature_importance)\n",
    "plt.title('Feature Importance via Lasso (L1) Logistic Regression')\n",
    "plt.show()\n",
    "\n",
    "# 4. Select Features\n",
    "# Keep features where coefficient is not 0\n",
    "selected_feats = feature_importance[feature_importance['Abs_Coef'] > 0]['Feature'].tolist()\n",
    "print(f\"Selected Features ({len(selected_feats)}): {selected_feats}\")\n",
    "\n",
    "# Update X to use only selected features for classification\n",
    "X_selected = X[selected_feats]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fdb876",
   "metadata": {
    "id": "f-Mpapro3D0j"
   },
   "source": [
    "Feature Selection Method 2: Mutual information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcb853a",
   "metadata": {
    "id": "CSb_0alG3Xyg"
   },
   "outputs": [],
   "source": [
    "# X is feature matrix, y is target\n",
    "mi = mutual_info_classif(X, y, random_state=42)\n",
    "\n",
    "# Convert MI scores into a readable format\n",
    "mi_scores = pd.Series(mi, index=X.columns)\n",
    "mi_scores = mi_scores.sort_values(ascending=False)\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Coefficient': mi_scores\n",
    "})\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(x='Coefficient', y='Feature', data=feature_importance)\n",
    "plt.title('Feature Importance via Mutual Information')\n",
    "plt.show()\n",
    "\n",
    "# Keep features where mutual informatoin is greater than 0.02\n",
    "selected_feats = feature_importance[feature_importance['Coefficient'] > 0.02]['Feature'].tolist()\n",
    "\n",
    "# Update X to use only selected features for classification\n",
    "X_selected_mi = X[selected_feats]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd067d7",
   "metadata": {
    "id": "3FCdMYZJ3YHZ"
   },
   "source": [
    "Feature Selection Method 3: RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492ad002",
   "metadata": {
    "id": "EK2nddZQ3bZn"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "X_RFE = data.drop(['ID', 'default.payment.next.month'], axis=1)\n",
    "y_RFE = data['default.payment.next.month']\n",
    "\n",
    "#Use logistic Regression as estimator\n",
    "\n",
    "rfe_sel = RFE(estimator = LogisticRegression(random_state=42, max_iter=1000), n_features_to_select=10)\n",
    "rfe_sel.fit(X_RFE,y_RFE)\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature' : X_RFE.columns,\n",
    "    'Ranking' : rfe_sel.ranking_\n",
    "})\n",
    "\n",
    "feature_importance = feature_importance.sort_values(by='Ranking', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.barplot(x='Ranking', y='Feature', data=feature_importance)\n",
    "plt.title('Feature Rankings via RFE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7206a86",
   "metadata": {
    "id": "V0SnFNXx2IaY"
   },
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb077ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(estimator, param_grid, classifier, X):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    grid_search = GridSearchCV(estimator=estimator, param_grid=param_grid,\n",
    "                            cv=5, n_jobs=-1, scoring='f1', verbose=1)\n",
    "\n",
    "    print(\"Starting Grid Search...\")\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "    best_rf = grid_search.best_estimator_\n",
    "\n",
    "    # 3. Evaluate Model\n",
    "    y_pred = best_rf.predict(X_test)\n",
    "    y_prob = best_rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    print(\"\\n--- Model Evaluation ---\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"Recall: {recall_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"F1-Score: {f1_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"AUC-ROC: {roc_auc_score(y_test, y_prob):.4f}\")\n",
    "\n",
    "    # Cross-Validation Consistency Check\n",
    "    cv_scores = cross_val_score(best_rf, X, y, cv=5, scoring='f1')\n",
    "    print(f\"\\n5-Fold CV F1-Scores: {cv_scores}\")\n",
    "    print(f\"Mean CV F1-Score: {cv_scores.mean():.4f}\")\n",
    "\n",
    "    # 4. Visualizations\n",
    "\n",
    "    # Confusion Matrix\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "    # ROC Curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, label=f'{classifier} (AUC = {roc_auc_score(y_test, y_prob):.2f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--') # Diagonal line\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62324f6",
   "metadata": {
    "id": "1_PeKLu02OKm"
   },
   "source": [
    "Classification Method 1: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3ba1d3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "h2aDAypq2LK5",
    "outputId": "70936543-c7be-45b5-cb65-477af3a8d556"
   },
   "outputs": [],
   "source": [
    "# 1. Split Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 2. Hyperparameter Tuning using GridSearchCV\n",
    "# Defining a smaller grid to save computation time; expand if needed\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid,\n",
    "                           cv=5, n_jobs=-1, scoring='f1', verbose=1)\n",
    "\n",
    "print(\"Starting Grid Search...\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "best_rf = grid_search.best_estimator_\n",
    "\n",
    "# 3. Evaluate Model\n",
    "y_pred = best_rf.predict(X_test)\n",
    "y_prob = best_rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"\\n--- Model Evaluation ---\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred):.4f}\")\n",
    "print(f\"F1-Score: {f1_score(y_test, y_pred):.4f}\")\n",
    "print(f\"AUC-ROC: {roc_auc_score(y_test, y_prob):.4f}\")\n",
    "\n",
    "# Cross-Validation Consistency Check\n",
    "cv_scores = cross_val_score(best_rf, X_selected, y, cv=5, scoring='f1')\n",
    "print(f\"\\n5-Fold CV F1-Scores: {cv_scores}\")\n",
    "print(f\"Mean CV F1-Score: {cv_scores.mean():.4f}\")\n",
    "\n",
    "# 4. Visualizations\n",
    "\n",
    "# Confusion Matrix\n",
    "plt.figure(figsize=(6, 5))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f'Random Forest (AUC = {roc_auc_score(y_test, y_prob):.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--') # Diagonal line\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f5261f",
   "metadata": {
    "id": "LUZg_ZEq2Rhk"
   },
   "source": [
    "Classification Method 2: k-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ab5045",
   "metadata": {
    "id": "39vtBLXh3o0z"
   },
   "outputs": [],
   "source": [
    "# WARNING: this cell takes a long time to run.\n",
    "param_grid = {\n",
    "    'n_neighbors': np.arange(2, 15),\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "    'leaf_size': np.arange(25, 35)\n",
    "}\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# rum model on regular X\n",
    "evaluate_model(estimator=knn, param_grid=param_grid, classifier=\"k-NN\", X=X)\n",
    "\n",
    "# run model on X with only selected features (mutual information)\n",
    "evaluate_model(estimator=knn, param_grid=param_grid, classifier=\"k-NN\", X=X_selected_mi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733266e0",
   "metadata": {
    "id": "jVmZyIkO3yzQ"
   },
   "source": [
    "Classification Method 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858eaa5a",
   "metadata": {
    "id": "TdMPUdNg31IT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8ec8b93",
   "metadata": {
    "id": "zZhYazvLGKEQ"
   },
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c63810",
   "metadata": {
    "id": "pg-9mSfaGVq6"
   },
   "source": [
    "Grid Search for Classification Method 1: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf85b256",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oOmE2LooGQie",
    "lines_to_next_cell": 2,
    "outputId": "fd024f8d-58e6-4da2-e1d6-478fdff3ed22"
   },
   "outputs": [],
   "source": [
    "\n",
    "# --- Step 1: Train Base Model (Before Tuning) ---\n",
    "print(\"Training Base Random Forest Model (Default Parameters)...\")\n",
    "# Initialize with default parameters\n",
    "base_rf = RandomForestClassifier(random_state=42)\n",
    "base_rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict using Base Model\n",
    "y_pred_base = base_rf.predict(X_test)\n",
    "y_prob_base = base_rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate Base Metrics\n",
    "base_accuracy = accuracy_score(y_test, y_pred_base)\n",
    "base_f1 = f1_score(y_test, y_pred_base)\n",
    "base_roc = roc_auc_score(y_test, y_prob_base)\n",
    "\n",
    "print(f\"Base Model Accuracy: {base_accuracy:.4f}\")\n",
    "print(f\"Base Model F1-Score: {base_f1:.4f}\")\n",
    "print(f\"Base Model AUC-ROC: {base_roc:.4f}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "\n",
    "# --- Step 2: Perform Hyperparameter Tuning (Grid Search) ---\n",
    "print(\"Starting Grid Search for Hyperparameter Tuning...\")\n",
    "\n",
    "# Define the parameter grid\n",
    "# We test different numbers of trees (n_estimators) and depths to find the best balance\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],       # Number of trees in the forest\n",
    "    'max_depth': [10, 20, None],          # Max depth of the tree\n",
    "    'min_samples_split': [2, 5, 10],      # Min samples required to split a node\n",
    "    'min_samples_leaf': [1, 2, 4]         # Min samples required at a leaf node\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "# cv=5 means 5-fold Cross-Validation\n",
    "# n_jobs=-1 uses all available CPU cores to speed up processing\n",
    "grid_search = GridSearchCV(estimator=RandomForestClassifier(random_state=42),\n",
    "                           param_grid=param_grid,\n",
    "                           cv=3,          # Lowered to 3 for speed, use 5 for final\n",
    "                           n_jobs=-1,\n",
    "                           scoring='f1',  # Optimize for F1 Score (good for imbalance)\n",
    "                           verbose=2)\n",
    "\n",
    "# Fit the Grid Search to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model\n",
    "best_rf = grid_search.best_estimator_\n",
    "print(f\"Best Parameters Found: {grid_search.best_params_}\")\n",
    "\n",
    "\n",
    "# --- Step 3: Evaluate Tuned Model ---\n",
    "print(\"-\" * 30)\n",
    "print(\"Evaluating Tuned Random Forest Model...\")\n",
    "\n",
    "# Predict using Tuned Model\n",
    "y_pred_tuned = best_rf.predict(X_test)\n",
    "y_prob_tuned = best_rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate Tuned Metrics\n",
    "tuned_accuracy = accuracy_score(y_test, y_pred_tuned)\n",
    "tuned_f1 = f1_score(y_test, y_pred_tuned)\n",
    "tuned_roc = roc_auc_score(y_test, y_prob_tuned)\n",
    "\n",
    "print(f\"Tuned Model Accuracy: {tuned_accuracy:.4f}\")\n",
    "print(f\"Tuned Model F1-Score: {tuned_f1:.4f}\")\n",
    "print(f\"Tuned Model AUC-ROC: {tuned_roc:.4f}\")\n",
    "\n",
    "\n",
    "# --- Step 4: Comparison & Discussion ---\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"IMPACT OF TUNING (Comparison)\")\n",
    "print(\"=\"*40)\n",
    "print(f\"{'Metric':<15} {'Base Model':<15} {'Tuned Model':<15} {'Improvement':<15}\")\n",
    "print(f\"{'-'*60}\")\n",
    "print(f\"{'Accuracy':<15} {base_accuracy:.4f}           {tuned_accuracy:.4f}           {tuned_accuracy - base_accuracy:+.4f}\")\n",
    "print(f\"{'F1-Score':<15} {base_f1:.4f}           {tuned_f1:.4f}           {tuned_f1 - base_f1:+.4f}\")\n",
    "print(f\"{'AUC-ROC':<15} {base_roc:.4f}           {tuned_roc:.4f}           {tuned_roc - base_roc:+.4f}\")\n",
    "print(\"=\"*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c43726c",
   "metadata": {
    "id": "k-lBEUT1GdGB"
   },
   "source": [
    "Grid/Random Search for Classification Method 2: k-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75afc1ff",
   "metadata": {
    "id": "MB2nMeExGJOp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57db7dd8",
   "metadata": {
    "id": "HUnF02E7GgdY"
   },
   "source": [
    "Grid/Random Search for Classification Method 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05225e67",
   "metadata": {
    "id": "YosfK2e_GmIn"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
